{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0-0bsNMMTnSZ",
    "outputId": "e1c336ac-1280-4b16-b1d4-c2971d6667c2"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"./FinRL-Library\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "matplotlib.use('Agg')\n",
    "import datetime\n",
    "\n",
    "from finrl.config import config\n",
    "from finrl.marketdata.yahoodownloader import YahooDownloader\n",
    "from finrl.preprocessing.preprocessors import FeatureEngineer\n",
    "from finrl.preprocessing.data import data_split\n",
    "from finrl.env.env_stocktrading import StockTradingEnv\n",
    "from finrl.model.models import DRLAgent\n",
    "from datetime import datetime,timedelta\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pos2IZAL54pp"
   },
   "source": [
    "<a id='1.4'></a>\n",
    "## 2.4. Create Folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "zp6lL6dZ53rX"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "if not os.path.exists(\"./\" + config.TENSORBOARD_LOG_DIR):\n",
    "    os.makedirs(\"./\" + config.TENSORBOARD_LOG_DIR)\n",
    "if not os.path.exists(\"./\" + config.RESULTS_DIR):\n",
    "    os.makedirs(\"./\" + config.RESULTS_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JtIFikNyTnSg",
    "outputId": "4c6a860f-6944-4728-984d-d0dc8d8f2c78"
   },
   "outputs": [],
   "source": [
    "col_names=['open','high','low','close']\n",
    "data_df = pd.read_csv('training.csv',names=col_names)\n",
    "data_df['tic']='IBM'\n",
    "base=datetime.strptime(config.START_DATE,\"%Y-%m-%d\")\n",
    "date=[base + timedelta(days=x)for x in range(len(data_df))]\n",
    "data_df['date']=date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 195
    },
    "id": "Vi6D_ED6TnSs",
    "outputId": "2313cd51-596f-4a58-922b-5ec837bbcbda"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "     open    high     low   close  tic       date\n",
       "0  186.73  188.71  186.00  186.30  IBM 1990-01-01\n",
       "1  185.57  186.33  184.94  185.54  IBM 1990-01-02\n",
       "2  184.81  185.03  183.10  184.66  IBM 1990-01-03\n",
       "3  184.39  184.48  182.31  182.54  IBM 1990-01-04\n",
       "4  182.20  182.27  180.27  181.59  IBM 1990-01-05"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>open</th>\n      <th>high</th>\n      <th>low</th>\n      <th>close</th>\n      <th>tic</th>\n      <th>date</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>186.73</td>\n      <td>188.71</td>\n      <td>186.00</td>\n      <td>186.30</td>\n      <td>IBM</td>\n      <td>1990-01-01</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>185.57</td>\n      <td>186.33</td>\n      <td>184.94</td>\n      <td>185.54</td>\n      <td>IBM</td>\n      <td>1990-01-02</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>184.81</td>\n      <td>185.03</td>\n      <td>183.10</td>\n      <td>184.66</td>\n      <td>IBM</td>\n      <td>1990-01-03</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>184.39</td>\n      <td>184.48</td>\n      <td>182.31</td>\n      <td>182.54</td>\n      <td>IBM</td>\n      <td>1990-01-04</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>182.20</td>\n      <td>182.27</td>\n      <td>180.27</td>\n      <td>181.59</td>\n      <td>IBM</td>\n      <td>1990-01-05</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rImfGAfCkR8j",
    "outputId": "5b55c2c7-3217-49ad-95b7-d9af0ad91d7a"
   },
   "outputs": [],
   "source": [
    "## user can add more technical indicators\n",
    "## check https://github.com/jealous/stockstats for different names\n",
    "#tech_indicator_list=config.TECHNICAL_INDICATORS_LIST\n",
    "tech_indicator_list=['macd','macds','macdh','kdjk','kdjd','close_5_sma','close_10_sma','close_20_sma','close_60_sma']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "etvRo2rSwZPg"
   },
   "source": [
    "<a id='3.2'></a>\n",
    "## 4.2 Perform Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SAOsx0m-9u2k",
    "outputId": "6238a0f8-e0bb-4468-ec75-ab3381ea09a9"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Successfully added technical indicators\n"
     ]
    }
   ],
   "source": [
    "fe = FeatureEngineer(\n",
    "                    use_technical_indicator=True,\n",
    "                    tech_indicator_list = tech_indicator_list,\n",
    "                    use_turbulence=False,\n",
    "                    user_defined_feature = False)\n",
    "\n",
    "data_df = fe.preprocess_data(data_df)\n",
    "\n",
    "train=data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 298
    },
    "id": "wytX_qwWMHP5",
    "outputId": "8113269b-39a0-462b-80b2-b8664eb27a66"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "        open    high     low   close  tic       date      macd     macds  \\\n",
       "1483  151.95  152.20  151.33  151.84  IBM 1994-01-23 -0.021715 -0.040454   \n",
       "1484  152.06  152.49  151.62  151.98  IBM 1994-01-24  0.077940 -0.016775   \n",
       "1485  152.35  152.93  151.70  152.47  IBM 1994-01-25  0.194217  0.025423   \n",
       "1486  152.81  153.61  152.17  153.55  IBM 1994-01-26  0.369258  0.094190   \n",
       "1487  153.65  154.41  153.08  153.97  IBM 1994-01-27  0.535694  0.182491   \n",
       "\n",
       "         macdh       kdjk       kdjd  close_5_sma  close_10_sma  close_20_sma  \\\n",
       "1483  0.018738  77.546456  60.081346      151.008       149.771      151.1415   \n",
       "1484  0.094715  82.323964  67.495552      151.610       150.053      151.0565   \n",
       "1485  0.168793  85.934230  73.641778      152.002       150.460      150.9620   \n",
       "1486  0.275067  90.295488  79.193015      152.322       150.926      150.9365   \n",
       "1487  0.353203  90.901890  83.095973      152.762       151.613      150.9335   \n",
       "\n",
       "      close_60_sma  \n",
       "1483    148.879000  \n",
       "1484    149.028167  \n",
       "1485    149.168000  \n",
       "1486    149.343167  \n",
       "1487    149.512333  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>open</th>\n      <th>high</th>\n      <th>low</th>\n      <th>close</th>\n      <th>tic</th>\n      <th>date</th>\n      <th>macd</th>\n      <th>macds</th>\n      <th>macdh</th>\n      <th>kdjk</th>\n      <th>kdjd</th>\n      <th>close_5_sma</th>\n      <th>close_10_sma</th>\n      <th>close_20_sma</th>\n      <th>close_60_sma</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1483</th>\n      <td>151.95</td>\n      <td>152.20</td>\n      <td>151.33</td>\n      <td>151.84</td>\n      <td>IBM</td>\n      <td>1994-01-23</td>\n      <td>-0.021715</td>\n      <td>-0.040454</td>\n      <td>0.018738</td>\n      <td>77.546456</td>\n      <td>60.081346</td>\n      <td>151.008</td>\n      <td>149.771</td>\n      <td>151.1415</td>\n      <td>148.879000</td>\n    </tr>\n    <tr>\n      <th>1484</th>\n      <td>152.06</td>\n      <td>152.49</td>\n      <td>151.62</td>\n      <td>151.98</td>\n      <td>IBM</td>\n      <td>1994-01-24</td>\n      <td>0.077940</td>\n      <td>-0.016775</td>\n      <td>0.094715</td>\n      <td>82.323964</td>\n      <td>67.495552</td>\n      <td>151.610</td>\n      <td>150.053</td>\n      <td>151.0565</td>\n      <td>149.028167</td>\n    </tr>\n    <tr>\n      <th>1485</th>\n      <td>152.35</td>\n      <td>152.93</td>\n      <td>151.70</td>\n      <td>152.47</td>\n      <td>IBM</td>\n      <td>1994-01-25</td>\n      <td>0.194217</td>\n      <td>0.025423</td>\n      <td>0.168793</td>\n      <td>85.934230</td>\n      <td>73.641778</td>\n      <td>152.002</td>\n      <td>150.460</td>\n      <td>150.9620</td>\n      <td>149.168000</td>\n    </tr>\n    <tr>\n      <th>1486</th>\n      <td>152.81</td>\n      <td>153.61</td>\n      <td>152.17</td>\n      <td>153.55</td>\n      <td>IBM</td>\n      <td>1994-01-26</td>\n      <td>0.369258</td>\n      <td>0.094190</td>\n      <td>0.275067</td>\n      <td>90.295488</td>\n      <td>79.193015</td>\n      <td>152.322</td>\n      <td>150.926</td>\n      <td>150.9365</td>\n      <td>149.343167</td>\n    </tr>\n    <tr>\n      <th>1487</th>\n      <td>153.65</td>\n      <td>154.41</td>\n      <td>153.08</td>\n      <td>153.97</td>\n      <td>IBM</td>\n      <td>1994-01-27</td>\n      <td>0.535694</td>\n      <td>0.182491</td>\n      <td>0.353203</td>\n      <td>90.901890</td>\n      <td>83.095973</td>\n      <td>152.762</td>\n      <td>151.613</td>\n      <td>150.9335</td>\n      <td>149.512333</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "train.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ipJcnLvQGAba",
    "outputId": "dc32ebc6-2191-4541-99d2-7e9861da5df6"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Stock Dimension: 1, State Space: 12\n"
     ]
    }
   ],
   "source": [
    "stock_dimension = 1\n",
    "state_space = 1 + 2*stock_dimension + len(tech_indicator_list)*stock_dimension\n",
    "print(f\"Stock Dimension: {stock_dimension}, State Space: {state_space}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "JY5wTwYjGTrg"
   },
   "outputs": [],
   "source": [
    "env_kwargs = {\n",
    "    \"hmax\": 1, \n",
    "    \"initial_amount\": 100000, \n",
    "    \"buy_cost_pct\": 0, \n",
    "    \"sell_cost_pct\": 0, \n",
    "    \"state_space\": state_space, \n",
    "    \"stock_dim\": stock_dimension, \n",
    "    \"tech_indicator_list\": tech_indicator_list, \n",
    "    \"action_space\": stock_dimension, \n",
    "    \"reward_scaling\": 1e-5\n",
    "\n",
    "}\n",
    "\n",
    "e_train_gym = StockTradingEnv(df = train, **env_kwargs,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vmyi2IPnyEkP",
    "outputId": "a968a96a-a1ee-426e-eed8-ee74d20b9bb8"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv'>\n"
     ]
    }
   ],
   "source": [
    "env_train, _ = e_train_gym.get_sb_env()\n",
    "print(type(env_train))\n",
    "agent = DRLAgent(env = env_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y9CM5DeIr9GC"
   },
   "source": [
    "### Model 1: A2C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rOk-Mr73-EEq",
    "outputId": "1c56a5c6-91ad-4b81-dd07-24ae69e64c23"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'\\nagent = DRLAgent(env = env_train)\\n\\nA2C_PARAMS = {\"n_steps\": 5, \"ent_coef\": 0.005, \"learning_rate\": 0.0002}\\nmodel_a2c = agent.get_model(model_name=\"a2c\",model_kwargs = A2C_PARAMS)\\n\\ntrained_a2c = agent.train_model(model=model_a2c, \\n                                tb_log_name=\\'a2c\\',\\n                                total_timesteps=50000)\\n'"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "'''\n",
    "agent = DRLAgent(env = env_train)\n",
    "\n",
    "A2C_PARAMS = {\"n_steps\": 5, \"ent_coef\": 0.005, \"learning_rate\": 0.0002}\n",
    "model_a2c = agent.get_model(model_name=\"a2c\",model_kwargs = A2C_PARAMS)\n",
    "\n",
    "trained_a2c = agent.train_model(model=model_a2c, \n",
    "                                tb_log_name='a2c',\n",
    "                                total_timesteps=50000)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9upN8FI2r_X1"
   },
   "source": [
    "### Model 2: PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gUFJlHsi-Ka-",
    "outputId": "1b7db70f-f740-401c-ab55-a567b4235e18"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'n_steps': 2048, 'ent_coef': 0.005, 'learning_rate': 0.0001, 'batch_size': 128}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/ppo/ppo_57\n",
      "----------------------------------\n",
      "| environment/        |          |\n",
      "|    portfolio_value  | 9.99e+04 |\n",
      "|    total_cost       | 0        |\n",
      "|    total_reward     | -68.5    |\n",
      "|    total_reward_pct | -0.0685  |\n",
      "|    total_trades     | 319      |\n",
      "| time/               |          |\n",
      "|    fps              | 637      |\n",
      "|    iterations       | 1        |\n",
      "|    time_elapsed     | 3        |\n",
      "|    total_timesteps  | 2048     |\n",
      "----------------------------------\n",
      "-----------------------------------------\n",
      "| environment/            |             |\n",
      "|    portfolio_value      | 1e+05       |\n",
      "|    total_cost           | 0           |\n",
      "|    total_reward         | -38.4       |\n",
      "|    total_reward_pct     | -0.0384     |\n",
      "|    total_trades         | 291         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 590         |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005878043 |\n",
      "|    clip_fraction        | 0.0361      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.42       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.0022      |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.00164    |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 0.000575    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| environment/            |             |\n",
      "|    portfolio_value      | 1e+05       |\n",
      "|    total_cost           | 0           |\n",
      "|    total_reward         | -22.8       |\n",
      "|    total_reward_pct     | -0.0228     |\n",
      "|    total_trades         | 324         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 581         |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007831708 |\n",
      "|    clip_fraction        | 0.0107      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.42       |\n",
      "|    explained_variance   | -1.19e-07   |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0168     |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.000736   |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 0.000415    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| environment/            |             |\n",
      "|    portfolio_value      | 1e+05       |\n",
      "|    total_cost           | 0           |\n",
      "|    total_reward         | -25.6       |\n",
      "|    total_reward_pct     | -0.0256     |\n",
      "|    total_trades         | 327         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 576         |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 14          |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003884092 |\n",
      "|    clip_fraction        | 0.0346      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.42       |\n",
      "|    explained_variance   | 5.96e-08    |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.016      |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.00175    |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 0.000532    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| environment/            |             |\n",
      "|    portfolio_value      | 9.99e+04    |\n",
      "|    total_cost           | 0           |\n",
      "|    total_reward         | -52.5       |\n",
      "|    total_reward_pct     | -0.0525     |\n",
      "|    total_trades         | 299         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 574         |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 17          |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003295848 |\n",
      "|    clip_fraction        | 4.88e-05    |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.42       |\n",
      "|    explained_variance   | -1.19e-07   |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0102     |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -6.34e-05   |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 0.000201    |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| environment/            |              |\n",
      "|    portfolio_value      | 1e+05        |\n",
      "|    total_cost           | 0            |\n",
      "|    total_reward         | 121          |\n",
      "|    total_reward_pct     | 0.121        |\n",
      "|    total_trades         | 253          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 573          |\n",
      "|    iterations           | 6            |\n",
      "|    time_elapsed         | 21           |\n",
      "|    total_timesteps      | 12288        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0003345157 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.42        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0001       |\n",
      "|    loss                 | -0.00774     |\n",
      "|    n_updates            | 50           |\n",
      "|    policy_gradient_loss | -6.32e-05    |\n",
      "|    std                  | 0.992        |\n",
      "|    value_loss           | 0.000137     |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| environment/            |             |\n",
      "|    portfolio_value      | 1e+05       |\n",
      "|    total_cost           | 0           |\n",
      "|    total_reward         | 107         |\n",
      "|    total_reward_pct     | 0.107       |\n",
      "|    total_trades         | 299         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 571         |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 25          |\n",
      "|    total_timesteps      | 14336       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009977229 |\n",
      "|    clip_fraction        | 0.0387      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.41       |\n",
      "|    explained_variance   | -1.19e-07   |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0115     |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.0017     |\n",
      "|    std                  | 0.99        |\n",
      "|    value_loss           | 0.000161    |\n",
      "-----------------------------------------\n",
      "--------------------------------------------\n",
      "| environment/            |                |\n",
      "|    portfolio_value      | 9.99e+04       |\n",
      "|    total_cost           | 0              |\n",
      "|    total_reward         | -146           |\n",
      "|    total_reward_pct     | -0.146         |\n",
      "|    total_trades         | 306            |\n",
      "| time/                   |                |\n",
      "|    fps                  | 571            |\n",
      "|    iterations           | 8              |\n",
      "|    time_elapsed         | 28             |\n",
      "|    total_timesteps      | 16384          |\n",
      "| train/                  |                |\n",
      "|    approx_kl            | -5.0866744e-05 |\n",
      "|    clip_fraction        | 4.88e-05       |\n",
      "|    clip_range           | 0.2            |\n",
      "|    entropy_loss         | -1.41          |\n",
      "|    explained_variance   | 0              |\n",
      "|    learning_rate        | 0.0001         |\n",
      "|    loss                 | -0.0112        |\n",
      "|    n_updates            | 70             |\n",
      "|    policy_gradient_loss | -0.000107      |\n",
      "|    std                  | 0.988          |\n",
      "|    value_loss           | 7.08e-05       |\n",
      "--------------------------------------------\n",
      "-------------------------------------------\n",
      "| environment/            |               |\n",
      "|    portfolio_value      | 1e+05         |\n",
      "|    total_cost           | 0             |\n",
      "|    total_reward         | 75.1          |\n",
      "|    total_reward_pct     | 0.0751        |\n",
      "|    total_trades         | 316           |\n",
      "| time/                   |               |\n",
      "|    fps                  | 570           |\n",
      "|    iterations           | 9             |\n",
      "|    time_elapsed         | 32            |\n",
      "|    total_timesteps      | 18432         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.6226975e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.41         |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0001        |\n",
      "|    loss                 | -0.00766      |\n",
      "|    n_updates            | 80            |\n",
      "|    policy_gradient_loss | 9.93e-05      |\n",
      "|    std                  | 0.986         |\n",
      "|    value_loss           | 7.26e-05      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| environment/            |               |\n",
      "|    portfolio_value      | 9.99e+04      |\n",
      "|    total_cost           | 0             |\n",
      "|    total_reward         | -84.2         |\n",
      "|    total_reward_pct     | -0.0842       |\n",
      "|    total_trades         | 326           |\n",
      "| time/                   |               |\n",
      "|    fps                  | 570           |\n",
      "|    iterations           | 10            |\n",
      "|    time_elapsed         | 35            |\n",
      "|    total_timesteps      | 20480         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00020764914 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.4          |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0001        |\n",
      "|    loss                 | -0.00632      |\n",
      "|    n_updates            | 90            |\n",
      "|    policy_gradient_loss | -0.000116     |\n",
      "|    std                  | 0.978         |\n",
      "|    value_loss           | 2.75e-05      |\n",
      "-------------------------------------------\n",
      "--------------------------------------------\n",
      "| environment/            |                |\n",
      "|    portfolio_value      | 1e+05          |\n",
      "|    total_cost           | 0              |\n",
      "|    total_reward         | 35.5           |\n",
      "|    total_reward_pct     | 0.0355         |\n",
      "|    total_trades         | 333            |\n",
      "| time/                   |                |\n",
      "|    fps                  | 570            |\n",
      "|    iterations           | 11             |\n",
      "|    time_elapsed         | 39             |\n",
      "|    total_timesteps      | 22528          |\n",
      "| train/                  |                |\n",
      "|    approx_kl            | -1.5400146e-05 |\n",
      "|    clip_fraction        | 0              |\n",
      "|    clip_range           | 0.2            |\n",
      "|    entropy_loss         | -1.4           |\n",
      "|    explained_variance   | 0              |\n",
      "|    learning_rate        | 0.0001         |\n",
      "|    loss                 | -0.00536       |\n",
      "|    n_updates            | 100            |\n",
      "|    policy_gradient_loss | 0.000102       |\n",
      "|    std                  | 0.978          |\n",
      "|    value_loss           | 2.03e-05       |\n",
      "--------------------------------------------\n",
      "-----------------------------------------\n",
      "| environment/            |             |\n",
      "|    portfolio_value      | 1e+05       |\n",
      "|    total_cost           | 0           |\n",
      "|    total_reward         | 57.4        |\n",
      "|    total_reward_pct     | 0.0574      |\n",
      "|    total_trades         | 303         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 569         |\n",
      "|    iterations           | 12          |\n",
      "|    time_elapsed         | 43          |\n",
      "|    total_timesteps      | 24576       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007972742 |\n",
      "|    clip_fraction        | 0.0357      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.4        |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.00385    |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.00205    |\n",
      "|    std                  | 0.975       |\n",
      "|    value_loss           | 2.24e-05    |\n",
      "-----------------------------------------\n",
      "-------------------------------------------\n",
      "| environment/            |               |\n",
      "|    portfolio_value      | 1e+05         |\n",
      "|    total_cost           | 0             |\n",
      "|    total_reward         | -21.5         |\n",
      "|    total_reward_pct     | -0.0215       |\n",
      "|    total_trades         | 288           |\n",
      "| time/                   |               |\n",
      "|    fps                  | 570           |\n",
      "|    iterations           | 13            |\n",
      "|    time_elapsed         | 46            |\n",
      "|    total_timesteps      | 26624         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00022737286 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.39         |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0001        |\n",
      "|    loss                 | -0.0103       |\n",
      "|    n_updates            | 120           |\n",
      "|    policy_gradient_loss | -0.000123     |\n",
      "|    std                  | 0.974         |\n",
      "|    value_loss           | 8.9e-06       |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| environment/            |               |\n",
      "|    portfolio_value      | 1e+05         |\n",
      "|    total_cost           | 0             |\n",
      "|    total_reward         | 70.9          |\n",
      "|    total_reward_pct     | 0.0709        |\n",
      "|    total_trades         | 308           |\n",
      "| time/                   |               |\n",
      "|    fps                  | 570           |\n",
      "|    iterations           | 14            |\n",
      "|    time_elapsed         | 50            |\n",
      "|    total_timesteps      | 28672         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00047406214 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.39         |\n",
      "|    explained_variance   | -1.19e-07     |\n",
      "|    learning_rate        | 0.0001        |\n",
      "|    loss                 | -0.00853      |\n",
      "|    n_updates            | 130           |\n",
      "|    policy_gradient_loss | -5.63e-05     |\n",
      "|    std                  | 0.969         |\n",
      "|    value_loss           | 5.92e-06      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| environment/            |               |\n",
      "|    portfolio_value      | 1e+05         |\n",
      "|    total_cost           | 0             |\n",
      "|    total_reward         | -27.8         |\n",
      "|    total_reward_pct     | -0.0278       |\n",
      "|    total_trades         | 295           |\n",
      "| time/                   |               |\n",
      "|    fps                  | 569           |\n",
      "|    iterations           | 15            |\n",
      "|    time_elapsed         | 53            |\n",
      "|    total_timesteps      | 30720         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | -0.0006161047 |\n",
      "|    clip_fraction        | 9.77e-05      |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.39         |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0001        |\n",
      "|    loss                 | -0.0108       |\n",
      "|    n_updates            | 140           |\n",
      "|    policy_gradient_loss | -0.000133     |\n",
      "|    std                  | 0.97          |\n",
      "|    value_loss           | 6.61e-06      |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| environment/            |              |\n",
      "|    portfolio_value      | 1e+05        |\n",
      "|    total_cost           | 0            |\n",
      "|    total_reward         | 41.2         |\n",
      "|    total_reward_pct     | 0.0412       |\n",
      "|    total_trades         | 259          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 569          |\n",
      "|    iterations           | 16           |\n",
      "|    time_elapsed         | 57           |\n",
      "|    total_timesteps      | 32768        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0032874777 |\n",
      "|    clip_fraction        | 0.00347      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.39        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0001       |\n",
      "|    loss                 | 2.53e-05     |\n",
      "|    n_updates            | 150          |\n",
      "|    policy_gradient_loss | -0.000345    |\n",
      "|    std                  | 0.967        |\n",
      "|    value_loss           | 2.53e-06     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| environment/            |              |\n",
      "|    portfolio_value      | 1e+05        |\n",
      "|    total_cost           | 0            |\n",
      "|    total_reward         | -33.8        |\n",
      "|    total_reward_pct     | -0.0338      |\n",
      "|    total_trades         | 279          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 569          |\n",
      "|    iterations           | 17           |\n",
      "|    time_elapsed         | 61           |\n",
      "|    total_timesteps      | 34816        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0024469094 |\n",
      "|    clip_fraction        | 0.000684     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.39        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0001       |\n",
      "|    loss                 | -0.00942     |\n",
      "|    n_updates            | 160          |\n",
      "|    policy_gradient_loss | -0.000234    |\n",
      "|    std                  | 0.972        |\n",
      "|    value_loss           | 3.25e-06     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| environment/            |              |\n",
      "|    portfolio_value      | 1e+05        |\n",
      "|    total_cost           | 0            |\n",
      "|    total_reward         | -9.48        |\n",
      "|    total_reward_pct     | -0.00948     |\n",
      "|    total_trades         | 251          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 569          |\n",
      "|    iterations           | 18           |\n",
      "|    time_elapsed         | 64           |\n",
      "|    total_timesteps      | 36864        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0073365513 |\n",
      "|    clip_fraction        | 0.0584       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.39        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0001       |\n",
      "|    loss                 | -0.025       |\n",
      "|    n_updates            | 170          |\n",
      "|    policy_gradient_loss | -0.00228     |\n",
      "|    std                  | 0.97         |\n",
      "|    value_loss           | 1.38e-06     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| environment/            |              |\n",
      "|    portfolio_value      | 1e+05        |\n",
      "|    total_cost           | 0            |\n",
      "|    total_reward         | 107          |\n",
      "|    total_reward_pct     | 0.107        |\n",
      "|    total_trades         | 191          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 568          |\n",
      "|    iterations           | 19           |\n",
      "|    time_elapsed         | 68           |\n",
      "|    total_timesteps      | 38912        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0061035594 |\n",
      "|    clip_fraction        | 0.0191       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.39        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0001       |\n",
      "|    loss                 | -0.0114      |\n",
      "|    n_updates            | 180          |\n",
      "|    policy_gradient_loss | -0.000317    |\n",
      "|    std                  | 0.969        |\n",
      "|    value_loss           | 1.07e-06     |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| environment/            |             |\n",
      "|    portfolio_value      | 1e+05       |\n",
      "|    total_cost           | 0           |\n",
      "|    total_reward         | -0.65       |\n",
      "|    total_reward_pct     | -0.00065    |\n",
      "|    total_trades         | 255         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 563         |\n",
      "|    iterations           | 20          |\n",
      "|    time_elapsed         | 72          |\n",
      "|    total_timesteps      | 40960       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006934586 |\n",
      "|    clip_fraction        | 0.0402      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.39       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.00208     |\n",
      "|    n_updates            | 190         |\n",
      "|    policy_gradient_loss | -0.00137    |\n",
      "|    std                  | 0.969       |\n",
      "|    value_loss           | 1.09e-06    |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| environment/            |              |\n",
      "|    portfolio_value      | 1e+05        |\n",
      "|    total_cost           | 0            |\n",
      "|    total_reward         | 64.9         |\n",
      "|    total_reward_pct     | 0.0649       |\n",
      "|    total_trades         | 231          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 561          |\n",
      "|    iterations           | 21           |\n",
      "|    time_elapsed         | 76           |\n",
      "|    total_timesteps      | 43008        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0066139917 |\n",
      "|    clip_fraction        | 0.0216       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.39        |\n",
      "|    explained_variance   | 1.19e-07     |\n",
      "|    learning_rate        | 0.0001       |\n",
      "|    loss                 | -0.0168      |\n",
      "|    n_updates            | 200          |\n",
      "|    policy_gradient_loss | -0.00114     |\n",
      "|    std                  | 0.966        |\n",
      "|    value_loss           | 4.19e-07     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| environment/            |              |\n",
      "|    portfolio_value      | 1e+05        |\n",
      "|    total_cost           | 0            |\n",
      "|    total_reward         | 30.5         |\n",
      "|    total_reward_pct     | 0.0305       |\n",
      "|    total_trades         | 197          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 559          |\n",
      "|    iterations           | 22           |\n",
      "|    time_elapsed         | 80           |\n",
      "|    total_timesteps      | 45056        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0041723964 |\n",
      "|    clip_fraction        | 0.00146      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.38        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0001       |\n",
      "|    loss                 | -0.00212     |\n",
      "|    n_updates            | 210          |\n",
      "|    policy_gradient_loss | -0.00013     |\n",
      "|    std                  | 0.965        |\n",
      "|    value_loss           | 3.04e-07     |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| environment/            |               |\n",
      "|    portfolio_value      | 1e+05         |\n",
      "|    total_cost           | 0             |\n",
      "|    total_reward         | 97.1          |\n",
      "|    total_reward_pct     | 0.0971        |\n",
      "|    total_trades         | 197           |\n",
      "| time/                   |               |\n",
      "|    fps                  | 558           |\n",
      "|    iterations           | 23            |\n",
      "|    time_elapsed         | 84            |\n",
      "|    total_timesteps      | 47104         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | -0.0002524899 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.38         |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0001        |\n",
      "|    loss                 | -0.0063       |\n",
      "|    n_updates            | 220           |\n",
      "|    policy_gradient_loss | 0.00013       |\n",
      "|    std                  | 0.965         |\n",
      "|    value_loss           | 3.94e-07      |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| environment/            |              |\n",
      "|    portfolio_value      | 1e+05        |\n",
      "|    total_cost           | 0            |\n",
      "|    total_reward         | 7.45         |\n",
      "|    total_reward_pct     | 0.00745      |\n",
      "|    total_trades         | 151          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 556          |\n",
      "|    iterations           | 24           |\n",
      "|    time_elapsed         | 88           |\n",
      "|    total_timesteps      | 49152        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0033356133 |\n",
      "|    clip_fraction        | 0.0121       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.38        |\n",
      "|    explained_variance   | 5.96e-08     |\n",
      "|    learning_rate        | 0.0001       |\n",
      "|    loss                 | -0.00997     |\n",
      "|    n_updates            | 230          |\n",
      "|    policy_gradient_loss | -0.000459    |\n",
      "|    std                  | 0.961        |\n",
      "|    value_loss           | 1.51e-07     |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| environment/            |             |\n",
      "|    portfolio_value      | 1e+05       |\n",
      "|    total_cost           | 0           |\n",
      "|    total_reward         | 20.3        |\n",
      "|    total_reward_pct     | 0.0203      |\n",
      "|    total_trades         | 156         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 555         |\n",
      "|    iterations           | 25          |\n",
      "|    time_elapsed         | 92          |\n",
      "|    total_timesteps      | 51200       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002262556 |\n",
      "|    clip_fraction        | 0.00313     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.37       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.00949    |\n",
      "|    n_updates            | 240         |\n",
      "|    policy_gradient_loss | -0.000652   |\n",
      "|    std                  | 0.95        |\n",
      "|    value_loss           | 1.61e-07    |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| environment/            |              |\n",
      "|    portfolio_value      | 1e+05        |\n",
      "|    total_cost           | 0            |\n",
      "|    total_reward         | -7.06        |\n",
      "|    total_reward_pct     | -0.00706     |\n",
      "|    total_trades         | 191          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 555          |\n",
      "|    iterations           | 26           |\n",
      "|    time_elapsed         | 95           |\n",
      "|    total_timesteps      | 53248        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0058934586 |\n",
      "|    clip_fraction        | 0.00942      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.36        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0001       |\n",
      "|    loss                 | 0.0064       |\n",
      "|    n_updates            | 250          |\n",
      "|    policy_gradient_loss | -0.000478    |\n",
      "|    std                  | 0.943        |\n",
      "|    value_loss           | 6.68e-08     |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| environment/            |             |\n",
      "|    portfolio_value      | 1e+05       |\n",
      "|    total_cost           | 0           |\n",
      "|    total_reward         | 8.81        |\n",
      "|    total_reward_pct     | 0.00881     |\n",
      "|    total_trades         | 259         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 555         |\n",
      "|    iterations           | 27          |\n",
      "|    time_elapsed         | 99          |\n",
      "|    total_timesteps      | 55296       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010589091 |\n",
      "|    clip_fraction        | 0.0666      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.36       |\n",
      "|    explained_variance   | 1.19e-07    |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.00857    |\n",
      "|    n_updates            | 260         |\n",
      "|    policy_gradient_loss | -0.00396    |\n",
      "|    std                  | 0.941       |\n",
      "|    value_loss           | 5.72e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------------\n",
      "| environment/            |           |\n",
      "|    portfolio_value      | 1e+05     |\n",
      "|    total_cost           | 0         |\n",
      "|    total_reward         | 62        |\n",
      "|    total_reward_pct     | 0.062     |\n",
      "|    total_trades         | 262       |\n",
      "| time/                   |           |\n",
      "|    fps                  | 556       |\n",
      "|    iterations           | 28        |\n",
      "|    time_elapsed         | 103       |\n",
      "|    total_timesteps      | 57344     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0038894 |\n",
      "|    clip_fraction        | 0.0132    |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -1.36     |\n",
      "|    explained_variance   | -1.19e-07 |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | 0.0178    |\n",
      "|    n_updates            | 270       |\n",
      "|    policy_gradient_loss | -0.00056  |\n",
      "|    std                  | 0.942     |\n",
      "|    value_loss           | 6.42e-08  |\n",
      "---------------------------------------\n",
      "-------------------------------------------\n",
      "| environment/            |               |\n",
      "|    portfolio_value      | 1e+05         |\n",
      "|    total_cost           | 0             |\n",
      "|    total_reward         | -10.1         |\n",
      "|    total_reward_pct     | -0.0101       |\n",
      "|    total_trades         | 244           |\n",
      "| time/                   |               |\n",
      "|    fps                  | 556           |\n",
      "|    iterations           | 29            |\n",
      "|    time_elapsed         | 106           |\n",
      "|    total_timesteps      | 59392         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.7201324e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.36         |\n",
      "|    explained_variance   | -1.19e-07     |\n",
      "|    learning_rate        | 0.0001        |\n",
      "|    loss                 | -0.00916      |\n",
      "|    n_updates            | 280           |\n",
      "|    policy_gradient_loss | 9.52e-05      |\n",
      "|    std                  | 0.943         |\n",
      "|    value_loss           | 2.28e-08      |\n",
      "-------------------------------------------\n",
      "-----------------------------------------\n",
      "| environment/            |             |\n",
      "|    portfolio_value      | 1e+05       |\n",
      "|    total_cost           | 0           |\n",
      "|    total_reward         | 72.4        |\n",
      "|    total_reward_pct     | 0.0724      |\n",
      "|    total_trades         | 311         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 557         |\n",
      "|    iterations           | 30          |\n",
      "|    time_elapsed         | 110         |\n",
      "|    total_timesteps      | 61440       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004931639 |\n",
      "|    clip_fraction        | 0.0231      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.36       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0149     |\n",
      "|    n_updates            | 290         |\n",
      "|    policy_gradient_loss | -0.00151    |\n",
      "|    std                  | 0.935       |\n",
      "|    value_loss           | 1.62e-08    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| environment/            |             |\n",
      "|    portfolio_value      | 1e+05       |\n",
      "|    total_cost           | 0           |\n",
      "|    total_reward         | -17.2       |\n",
      "|    total_reward_pct     | -0.0172     |\n",
      "|    total_trades         | 276         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 555         |\n",
      "|    iterations           | 31          |\n",
      "|    time_elapsed         | 114         |\n",
      "|    total_timesteps      | 63488       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002714014 |\n",
      "|    clip_fraction        | 0.00352     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.35       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.00372     |\n",
      "|    n_updates            | 300         |\n",
      "|    policy_gradient_loss | -0.000284   |\n",
      "|    std                  | 0.936       |\n",
      "|    value_loss           | 2.11e-08    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| environment/            |             |\n",
      "|    portfolio_value      | 1e+05       |\n",
      "|    total_cost           | 0           |\n",
      "|    total_reward         | 38.3        |\n",
      "|    total_reward_pct     | 0.0383      |\n",
      "|    total_trades         | 264         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 554         |\n",
      "|    iterations           | 32          |\n",
      "|    time_elapsed         | 118         |\n",
      "|    total_timesteps      | 65536       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008392959 |\n",
      "|    clip_fraction        | 0.0528      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.35       |\n",
      "|    explained_variance   | 5.96e-08    |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.00522    |\n",
      "|    n_updates            | 310         |\n",
      "|    policy_gradient_loss | -0.00303    |\n",
      "|    std                  | 0.931       |\n",
      "|    value_loss           | 8.71e-09    |\n",
      "-----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "agent = DRLAgent(env = env_train)\n",
    "PPO_PARAMS = {\n",
    "    \"n_steps\": 2048,\n",
    "    \"ent_coef\": 0.005,\n",
    "    \"learning_rate\": 0.0001,\n",
    "    \"batch_size\": 128,\n",
    "}\n",
    "model_ppo = agent.get_model(\"ppo\",model_kwargs = PPO_PARAMS)\n",
    "trained_ppo = agent.train_model(model=model_ppo, \n",
    "                            tb_log_name='ppo',\n",
    "                            total_timesteps=60000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Successfully added technical indicators\n",
      "    0\n",
      "0   1\n",
      "1   0\n",
      "2   0\n",
      "3   0\n",
      "4   0\n",
      "5   0\n",
      "6   0\n",
      "7   0\n",
      "8  -1\n",
      "9   0\n",
      "10  0\n",
      "11  0\n",
      "12  1\n",
      "13  0\n",
      "14  0\n",
      "15  0\n",
      "16  0\n",
      "17  0\n",
      "18  0\n",
      "99999.34000000001\n",
      "70.0%\n"
     ]
    }
   ],
   "source": [
    "trade = pd.read_csv('testing.csv',names=col_names)\n",
    "trade['tic']='IBM'\n",
    "base=datetime.strptime(config.START_TRADE_DATE,\"%Y-%m-%d\")\n",
    "date=[base + timedelta(days=x)for x in range(len(trade))]\n",
    "trade['date']=date\n",
    "trade=fe.preprocess_data(trade)\n",
    "\n",
    "e_trade_gym = StockTradingEnv(df = trade, **env_kwargs)\n",
    "\n",
    "max_profit=0\n",
    "actions=None\n",
    "negative_trade=0\n",
    "total_testing_num=10\n",
    "for _ in range(total_testing_num):\n",
    "    df_account_value, df_actions = DRLAgent.DRL_prediction(model=trained_ppo,environment = e_trade_gym)\n",
    "    final_profit=df_account_value['account_value'].iloc[-1]\n",
    "    if final_profit<100000:\n",
    "        negative_trade+=1\n",
    "    \n",
    "df_account_value, df_actions = DRLAgent.DRL_prediction(model=trained_ppo,environment = e_trade_gym)\n",
    "final_profit=df_account_value['account_value'].iloc[-1]\n",
    "actions=pd.DataFrame(np.array(df_actions['actions'],dtype='int'))\n",
    "print(actions)\n",
    "print(final_profit)\n",
    "print(f\"{(negative_trade/total_testing_num)*100}%\")\n",
    "actions.to_csv(\"output.csv\",index=False,header=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "Y9CM5DeIr9GC",
    "9upN8FI2r_X1",
    "CiBG2ZknsG73"
   ],
   "include_colab_link": true,
   "name": "FinRL_single_stock_trading.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}